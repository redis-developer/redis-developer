"use strict";(self.webpackChunkredis_developer_hub=self.webpackChunkredis_developer_hub||[]).push([[6325],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},s=Object.keys(e);for(i=0;i<s.length;i++)n=s[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(i=0;i<s.length;i++)n=s[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),d=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=d(e.components);return i.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},h=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),h=d(n),u=a,m=h["".concat(l,".").concat(u)]||h[u]||p[u]||s;return n?i.createElement(m,o(o({ref:t},c),{},{components:n})):i.createElement(m,o({ref:t},c))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,o=new Array(s);o[0]=h;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:a,o[1]=r;for(var d=2;d<s;d++)o[d]=n[d];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}h.displayName="MDXCreateElement"},50358:(e,t,n)=>{n.d(t,{Z:()=>l});var i=n(67294),a=n(52263);const s="authorByline_VoxI",o="authorLabel_a70t",r="authorProfileImage_URwT";const l=function(e){let{frontMatter:t}=e;const{siteConfig:n}=(0,a.Z)(),l=n.customFields.authors;return i.createElement(i.Fragment,null,t.authors&&i.createElement("div",{className:"docAuthors"},i.createElement("hr",null),t.authors.map((e=>i.createElement("div",{key:e,className:s},i.createElement("img",{className:r,src:`/img/${l[e].image?l[e].image:"default_author_profile_pic.png"}`,alt:`Profile picture for ${l[e].name}`}),i.createElement("div",null,i.createElement("div",{className:o},"Author:"),i.createElement("div",null,i.createElement("a",{href:l[e].link,target:"_blank"},l[e].name),", ",l[e].title))))),i.createElement("hr",null)))}},89393:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var i=n(87462),a=(n(67294),n(3905)),s=n(50358);const o={id:"index-video-qa",title:"Building an AI-Powered Video Q&A Application with Redis and LangChain",sidebar_label:"Building an AI-Powered Video Q&A Application with Redis and LangChain",slug:"/howtos/solutions/vector/ai-qa-videos-langchain-redis-openai-google",authors:["prasan","will"]},r=void 0,l={unversionedId:"howtos/solutions/vector/video-qa/index-video-qa",id:"howtos/solutions/vector/video-qa/index-video-qa",title:"Building an AI-Powered Video Q&A Application with Redis and LangChain",description:"What you will learn in this tutorial",source:"@site/docs/howtos/solutions/vector/video-qa/index-video-qa.mdx",sourceDirName:"howtos/solutions/vector/video-qa",slug:"/howtos/solutions/vector/ai-qa-videos-langchain-redis-openai-google",permalink:"/howtos/solutions/vector/ai-qa-videos-langchain-redis-openai-google",draft:!1,editUrl:"https://github.com/redis-developer/redis-developer/edit/master/docs/howtos/solutions/vector/video-qa/index-video-qa.mdx",tags:[],version:"current",lastUpdatedAt:1706572813,formattedLastUpdatedAt:"Jan 30, 2024",frontMatter:{id:"index-video-qa",title:"Building an AI-Powered Video Q&A Application with Redis and LangChain",sidebar_label:"Building an AI-Powered Video Q&A Application with Redis and LangChain",slug:"/howtos/solutions/vector/ai-qa-videos-langchain-redis-openai-google",authors:["prasan","will"]},sidebar:"docs",previous:{title:"Semantic Image Based Queries Using LangChain (OpenAI) and Redis",permalink:"/howtos/solutions/vector/image-summary-search"},next:{title:"Getting Started With Triggers and Functions in Redis",permalink:"/howtos/solutions/triggers-and-functions/getting-started"}},d={},c=[{value:"What you will learn in this tutorial",id:"what-you-will-learn-in-this-tutorial",level:2},{value:"Introduction",id:"introduction",level:2},{value:"High-level overview of the AI video Q&amp;A application with Redis",id:"high-level-overview-of-the-ai-video-qa-application-with-redis",level:2},{value:"Setting Up the Environment",id:"setting-up-the-environment",level:2},{value:"Requirements",id:"requirements",level:3},{value:"Setting Up Redis",id:"setting-up-redis",level:3},{value:"Cloning the Repository",id:"cloning-the-repository",level:3},{value:"Installing Dependencies",id:"installing-dependencies",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Running the application",id:"running-the-application",level:2},{value:"How to build a video Q&amp;A application with Redis and LangChain",id:"how-to-build-a-video-qa-application-with-redis-and-langchain",level:2},{value:"Video uploading and processing",id:"video-uploading-and-processing",level:3},{value:"Handling video uploads and retreiving video transcripts and metadata",id:"handling-video-uploads-and-retreiving-video-transcripts-and-metadata",level:4},{value:"Summarizing video content with LangChain, Redis, Google Gemini, and OpenAI ChatGPT",id:"summarizing-video-content-with-langchain-redis-google-gemini-and-openai-chatgpt",level:4},{value:"Redis vector search funcationality and AI integration for video Q&amp;A",id:"redis-vector-search-funcationality-and-ai-integration-for-video-qa",level:3},{value:"Converting questions into vectors",id:"converting-questions-into-vectors",level:4},{value:"Conclusion",id:"conclusion",level:2},{value:"Key takeaways",id:"key-takeaways",level:3},{value:"Further reading",id:"further-reading",level:2}],p={toc:c};function h(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,i.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)(s.Z,{frontMatter:o,mdxType:"Authors"}),(0,a.kt)("h2",{id:"what-you-will-learn-in-this-tutorial"},"What you will learn in this tutorial"),(0,a.kt)("p",null,"This tutorial focuses on building a Q&A answer engine for video content. It will cover the following topics:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"How to use ",(0,a.kt)("inlineCode",{parentName:"li"},"OpenAI"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"Google Gemini"),", and ",(0,a.kt)("inlineCode",{parentName:"li"},"LangChain")," to summarize videos content and generate vector embeddings"),(0,a.kt)("li",{parentName:"ol"},"How to use ",(0,a.kt)("inlineCode",{parentName:"li"},"Redis")," to store and search vector embeddings"),(0,a.kt)("li",{parentName:"ol"},"How to use ",(0,a.kt)("inlineCode",{parentName:"li"},"Redis")," as a semantic vector search cache")),(0,a.kt)("admonition",{title:"GITHUB CODE",type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"Below is a command to the clone the source code for the application used in this tutorial"),(0,a.kt)("pre",{parentName:"admonition"},(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/wjohnsto/genai-qa-videos\n"))),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"Before we dive into the details of this tutorial, let's go over a few concepts that are important to understand when building generative AI applications."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Generative AI")," is a rapidly evolving field that focuses on creating content, whether it's text, images, or even video. It leverages deep learning techniques to generate new, unique outputs based on learned patterns and data."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Retrieval-Augmented Generation (RAG)")," combines generative models with external knowledge sources to provide more accurate and informed responses. This technique is particularly useful in applications where context-specific information is critical."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"https://www.langchain.com/"},(0,a.kt)("strong",{parentName:"a"},"LangChain"))," is a powerful library that facilitates the development of applications involving language models. It simplifies tasks such as summarization, question answering, and interaction with generative models like ChatGPT or Google Gemini."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Google Gemini")," and ",(0,a.kt)("strong",{parentName:"li"},"OpenAI/ChatGPT")," are generative models that can be used to generate text based on a given prompt. They are useful for applications that require a large amount of text generation, such as summarization or question answering."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Semantic vector search")," is a technique that uses vector embeddings to find similar items in a database. It is typically combined with RAG to provide more accurate responses to user queries."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Redis")," is an in-memory database that can be used to store and search vector embeddings. It is particularly useful for applications that require fast, real-time responses.")),(0,a.kt)("p",null,"Our application leverages these technologies to create a unique Q&A platform based on video content. Users can upload YouTube video URLs or IDs, and the application utilizes generative AI to summarize these videos, formulate potential questions, and create a searchable database. This database can then be queried to find answers to user-submitted questions, drawing directly from the video content."),(0,a.kt)("h2",{id:"high-level-overview-of-the-ai-video-qa-application-with-redis"},"High-level overview of the AI video Q&A application with Redis"),(0,a.kt)("p",null,"Here's how our application uses AI and semantic vector search to answer user questions based on video content:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Uploading videos"),": Users can upload YouTube videos either via links (e.g. ",(0,a.kt)("inlineCode",{parentName:"li"},"https://www.youtube.com/watch?v=LaiQFZ5bXaM"),") or video IDs (e.g. ",(0,a.kt)("inlineCode",{parentName:"li"},"LaiQFZ5bXaM"),"). The application processes these inputs to retrieve necessary video information. For the purposes of this tutorial, the app is pre-seeded with a collection of videos from the ",(0,a.kt)("a",{parentName:"li",href:"https://www.youtube.com/@Redisinc"},"Redis YouTube channel"),".")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Upload videos screenshot",src:n(77723).Z,width:"919",height:"312"}),"."),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Video processing and AI interaction"),": Using the ",(0,a.kt)("a",{parentName:"li",href:"https://developers.google.com/youtube/v3"},"Youtube Data API"),", the application obtains video ",(0,a.kt)("inlineCode",{parentName:"li"},"titles"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"descriptions"),", and ",(0,a.kt)("inlineCode",{parentName:"li"},"thumbnails"),". It also uses ",(0,a.kt)("a",{parentName:"li",href:"https://searchapi.io"},"SearchAPI.io")," to retrieve video transcripts. These transcripts are then passed to a large language model (LLM) - either Google Gemini or OpenAI's ChatGPT - for summarization and sample question generation. The LLM also generates vector embeddings for these summaries.")),(0,a.kt)("p",null,"An example summary and sample questions generated by the LLM are shown below:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="https://www.youtube.com/watch?v=LaiQFZ5bXaM"',title:'"https://www.youtube.com/watch?v'},"Summary:\nThe video provides a walkthrough of building a real-time stock tracking application\nusing Redis Stack, demonstrating its capability to handle multiple data models and\nact as a message broker in a single integrated database. The application maintains\na watch list of stock symbols, along with real-time trading information and a chart\nupdated with live data from the Alpaca API. The presenter uses Redis Stack features\nsuch as sets, JSON documents, time series, Pub/Sub, and Top-K filter to store and\nmanage different types of data. An architecture diagram is provided, explaining the\ninterconnection between the front end, API service, and streaming service within\nthe application. Code snippets highlight key aspects of the API and streaming\nservice written in Python, highlighting the use of Redis Bloom, Redis JSON, Redis\nTime Series, and Redis Search for managing data. The video concludes with a\ndemonstration of how data structures are visualized and managed in RedisInsight,\nemphasizing how Redis Stack can simplify the building of a complex real-time\napplication by replacing multiple traditional technologies with one solution.\n\nExample Questions and Answers:\n\nQ1: What is Redis Stack and what role does it play in the application?\nA1: Redis Stack is an extension to Redis that adds additional modules, turning it\ninto a multi-model database. In the application, it is used for storing various\ntypes of data and managing real-time communication between microservices.\n\nQ2: How is the stock watch list stored and managed within the application?\nA2: The watch list is stored as a Redis set which helps automatically prevent\nduplicate stock symbols. In addition, further information about each stock is\nstored in JSON documents within Redis Stack.\n\nQ3: What type of data does the application store using time series capabilities of\nRedis Stack?\nA3: The application uses time series data to store and retrieve the price movements\nof the stocks, making it easy to query over a date range and to visualize chart\ndata with time on the x-axis.\n\nQ4: Can you explain the use of the Top-K filter in the application?\nA4: The Top-K filter is a feature of Redis Bloom that is used to maintain a\nleaderboard of the most frequently traded stocks on the watch list, updating every\nminute with the number of trades that happen.\n\nQ5: What methods are used to update the front end with real-time information in\nthe application?\nA5: WebSockets are used to receive real-time updates for trending stocks, trades,\nand stock bars from the API service, which, in turn, receives the information from\nRedis Pub/Sub messages generated by the streaming service.\n\nQ6: How does the application sync the watch list with the streaming service?\nA6: The application listens to the watch list key space in Redis for updates. When\na stock is added or removed from the watch list on the front end, the API\ncommunicates this to Redis, and the streaming service then subscribes or\nunsubscribes to updates from the Alpaca API for that stock.\n\nQ7: What frontend technologies are mentioned for building the UI of the application?\nA7: The UI service for the front end is built using Tailwind CSS, Chart.js, and\nNext.js, which is a typical tech stack for creating a modern web application.\n\nQ8: How does Redis Insight help in managing the application data?\nA8: Redis Insight provides a visual interface to see and manage the data structures\nused in Redis Stack, including JSON documents, sets, and time series data related\nto the stock information in the application.\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Data storage with Redis"),": All generated data, including video summaries, potential questions, and vector embeddings, are stored in Redis. The app utilizes Redis's diverse data types for efficient data handling, caching, and quick retrieval.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"RedisInsight keys",src:n(37800).Z,width:"1622",height:"801"})),(0,a.kt)("ol",{start:4},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Search and answer retrieval"),": The frontend, built with Next.js, allows users to ask questions. The application then searches the Redis database using semantic vector similarity to find relevant video content. It further uses the LLM to formulate answers, prioritizing information from video transcripts.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Asking a question",src:n(21540).Z,width:"1061",height:"389"})),(0,a.kt)("ol",{start:5},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Presentation of results"),": The app displays the most relevant videos along with the AI-generated answers, offering a comprehensive and interactive user experience. It also displays cached results from previous queries using semantic vector caching for faster response times.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Existing answers",src:n(91050).Z,width:"3060",height:"1308"})),(0,a.kt)("h2",{id:"setting-up-the-environment"},"Setting Up the Environment"),(0,a.kt)("p",null,"To get started with our AI-powered video Q&A application, you'll first need to set up your development environment. We'll follow the instructions outlined in the project's ",(0,a.kt)("inlineCode",{parentName:"p"},"README.md")," file."),(0,a.kt)("h3",{id:"requirements"},"Requirements"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://nodejs.org/"},"Node.js")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://www.docker.com/"},"Docker")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://www.searchapi.io/"},"SearchAPI.io API Key"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"This is used to retrieve video transcripts and free for up to 100 requests. The application will cache the results to help avoid exceeding the free tier."))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/apis/credentials"},"Google API Key"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"You must have the following APIs enabled:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"YouTube Data API v3"),(0,a.kt)("li",{parentName:"ul"},"Generative Language API"))),(0,a.kt)("li",{parentName:"ul"},"This is used to retrieve video information and prompt the Google Gemini model. This is not free."))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://platform.openai.com/api-keys"},"OpenAI API Key"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"This is used to prompt the OpenAI ChatGPT model. This is not free.")))),(0,a.kt)("h3",{id:"setting-up-redis"},"Setting Up Redis"),(0,a.kt)("p",null,"Redis is used as our database to store and retrieve data efficiently. You can start quickly with a cloud-hosted Redis instance by signing up at redis.com/try-free. This is ideal for both development and testing purposes. You can easily store the data for this application within the limitations of the Redis free tier."),(0,a.kt)("h3",{id:"cloning-the-repository"},"Cloning the Repository"),(0,a.kt)("p",null,"First, clone the repository containing our project:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/wjohnsto/genai-qa-videos\n")),(0,a.kt)("h3",{id:"installing-dependencies"},"Installing Dependencies"),(0,a.kt)("p",null,"After setting up your Node.js environment, you'll need to install the necessary packages. Navigate to the root of your project directory and run the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"npm install\n")),(0,a.kt)("p",null,"This command will install all the dependencies listed in the ",(0,a.kt)("inlineCode",{parentName:"p"},"package.json")," file, ensuring you have everything needed to run the application."),(0,a.kt)("h3",{id:"configuration"},"Configuration"),(0,a.kt)("p",null,"Before running the application, make sure to configure the environment variables. There is a script to automatically generate the ",(0,a.kt)("inlineCode",{parentName:"p"},".env")," files for you. Run the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"npm run setup\n")),(0,a.kt)("p",null,"This will generate the following files:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"app/.env")," - This file contains the environment variables for the Next.js application."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"app/.env.docker")," - This file contains overrides for the environment variables when running in Docker."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"services/video-search/.env")," - This file contains the environment variables for the video search service."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"services/video-search/.env.docker")," - This file contains overrides for the environment variables when running in Docker.")),(0,a.kt)("p",null,"By default, you should not need to touch the environment files in the ",(0,a.kt)("inlineCode",{parentName:"p"},"app"),". However, you will need to configure the environment files in the ",(0,a.kt)("inlineCode",{parentName:"p"},"services/video-search")," directory."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"services/video-search/.env")," looks like this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"USE=<HF|OPENAI>\n\nREDIS_URL=<redis[s]://[[username][:password]@][host][:port][/db-number]>\nSEARCHAPI_API_KEY=<https://www.searchapi.io/>\nYOUTUBE_TRANSCRIPT_PREFIX=<redis-transcript-prefix>\nYOUTUBE_VIDEO_INFO_PREFIX=<redis-video-info-prefix>\n\nGOOGLE_API_KEY=<https://console.cloud.google.com/apis/credentials>\nGOOGLE_EMBEDDING_MODEL=<https://ai.google.dev/models/gemini#model_variations>\nGOOGLE_SUMMARY_MODEL=<https://ai.google.dev/models/gemini#model_variations>\n\nOPENAI_API_KEY=<https://platform.openai.com/api-keys>\nOPENAI_ORGANIZATION=<https://platform.openai.com/account/organization>\nOPENAI_EMBEDDING_MODEL=<https://platform.openai.com/account/limits>\nOPENAI_SUMMARY_MODEL=<https://platform.openai.com/account/limits>\n")),(0,a.kt)("p",null,"For Gemini models, you can use the following if you are not sure what to do:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"GOOGLE_EMBEDDING_MODEL=embedding-001\nGOOGLE_SUMMARY_MODEL=gemini-pro\n")),(0,a.kt)("p",null,"For OpenAI models, you can use the following if you are not sure what to do:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"OPENAI_EMBEDDING_MODEL=text-embedding-ada-002\nOPENAI_SUMMARY_MODEL=gpt-4-1106-preview\n")),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"NOTE: Depending on your OpenAI tier you may have to use a different summary model. ",(0,a.kt)("inlineCode",{parentName:"p"},"gpt-3.5")," models will be okay.")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"_PREFIX")," environment variables are used to prefix the keys in Redis. This is useful if you want to use the same Redis instance for multiple applications. THey have the following defaults:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"YOUTUBE_TRANSCRIPT_PREFIX=transcripts:\nYOUTUBE_VIDEO_INFO_PREFIX=yt-videos:\n")),(0,a.kt)("p",null,"If you're satisfied with the defaults, you can delete these values from the ",(0,a.kt)("inlineCode",{parentName:"p"},".env")," file."),(0,a.kt)("p",null,"Lastly, the ",(0,a.kt)("inlineCode",{parentName:"p"},"services/video-search/.env.docker")," file contains overrides for the Redis URL when used in Docker. By default this app sets up a local Redis instance in Docker. If you are using a cloud instance, you can simply add the URL to your ",(0,a.kt)("inlineCode",{parentName:"p"},".env")," and delete the override in the ",(0,a.kt)("inlineCode",{parentName:"p"},".env.docker")," file."),(0,a.kt)("h2",{id:"running-the-application"},"Running the application"),(0,a.kt)("p",null,"After installing and configuring the application, run the following command to build the Docker images and run containers:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"npm run dev\n")),(0,a.kt)("p",null,"This command builds the app and the video service, and deploys them to Docker. It is all setup for hot reloading, so if you make changes to the code, it will automatically restart the servers."),(0,a.kt)("p",null,"Once the containers are up and running, the application will be accessible via your web browser:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Client"),": Available at ",(0,a.kt)("a",{parentName:"li",href:"http://localhost"},"http://localhost")," (Port 80)."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Video search service"),": Accessible at ",(0,a.kt)("a",{parentName:"li",href:"http://localhost:8000/api/healthcheck"},"http://localhost:8000"),".")),(0,a.kt)("p",null,"This setup allows you to interact with the client-side application through your browser and make requests to the video search service hosted on a separate port."),(0,a.kt)("p",null,"The video search service doesn't publish a client application. Instead, it exposes a REST API that can be used to interact with the service. You can validate that it is running by checking Docker or by visiting the following URL:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"http://localhost:8000/api/healthcheck"},"http://localhost:8000/api/healthcheck"))),(0,a.kt)("p",null,"You should be up and running now! The rest of this tutorial is focused on how the application works and how to use it, with code examples."),(0,a.kt)("h2",{id:"how-to-build-a-video-qa-application-with-redis-and-langchain"},"How to build a video Q&A application with Redis and LangChain"),(0,a.kt)("h3",{id:"video-uploading-and-processing"},"Video uploading and processing"),(0,a.kt)("h4",{id:"handling-video-uploads-and-retreiving-video-transcripts-and-metadata"},"Handling video uploads and retreiving video transcripts and metadata"),(0,a.kt)("p",null,"The backend is set up to handle YouTube video links or IDs. The relevant code snippet from the project demonstrates how these inputs are processed."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/transcripts/load.ts"',title:'"services/video-search/src/transcripts/load.ts"'},"export type VideoDocument = Document<{\n  id: string;\n  link: string;\n  title: string;\n  description: string;\n  thumbnail: string;\n}>;\n\nexport async function load(videos: string[] = config.youtube.VIDEOS) {\n  // Parse the video URLs to get a list of video IDs\n  const videosToLoad: string[] = videos.map(parseVideoUrl).filter((video) => {\n    return typeof video === 'string';\n  }) as string[];\n\n  // Get video title, description, and thumbail from YouTube API v3\n  const videoInfo = await getVideoInfo(videosToLoad);\n\n  // Get video transcripts from SearchAPI.io, join the video info\n  const transcripts = await mapAsyncInOrder(videosToLoad, async (video) => {\n    return await getTranscript(video, videoInfo[video]);\n  });\n\n  // Return the videos as documents with metadata, and pageContent being the transcript\n  return transcripts.filter(\n    (transcript) => typeof transcript !== 'undefined',\n  ) as VideoDocument[];\n}\n")),(0,a.kt)("p",null,"In the same file you will see two caches:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/transcripts/load.ts"',title:'"services/video-search/src/transcripts/load.ts"'},"const cache = cacheAside(config.youtube.TRANSCRIPT_PREFIX);\nconst videoCache = jsonCacheAside<VideoInfo>(config.youtube.VIDEO_INFO_PREFIX);\n")),(0,a.kt)("p",null,"These caches are used to store the transcripts (as a ",(0,a.kt)("inlineCode",{parentName:"p"},"string"),") and video metadata (as ",(0,a.kt)("inlineCode",{parentName:"p"},"JSON"),") in Redis. The ",(0,a.kt)("inlineCode",{parentName:"p"},"cache")," functions are helper functions that use Redis to store and retrieve data. They looks like this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/db.ts"',title:'"services/video-search/src/db.ts"'},"export function cacheAside(prefix: string) {\n  return {\n    get: async (key: string) => {\n      return await client.get(`${prefix}${key}`);\n    },\n    set: async (key: string, value: string) => {\n      return await client.set(`${prefix}${key}`, value);\n    },\n  };\n}\n\nexport function jsonCacheAside<T>(prefix: string) {\n  return {\n    get: async (key: string): Promise<T | undefined> => {\n      return client.json.get(`${prefix}${key}`) as T;\n    },\n    set: async (key: string, value: RedisJSON) => {\n      return await client.json.set(`${prefix}${key}`, '$', value);\n    },\n  };\n}\n")),(0,a.kt)("p",null,"You will see these functions used elsewhere in the app. They are used to prevent unnecessary API calls, in this case to SearchAPI.io and the YouTube API."),(0,a.kt)("h4",{id:"summarizing-video-content-with-langchain-redis-google-gemini-and-openai-chatgpt"},"Summarizing video content with LangChain, Redis, Google Gemini, and OpenAI ChatGPT"),(0,a.kt)("p",null,"After obtaining the video transcripts and metadata, the transcripts are then summarized using LangChain and the LLMs, both Gemini and ChatGPT. There are a few interesting pieces of code to understand here:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"prompt")," used to ask the LLM to summarize the video transcript and generate sample questions"),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"refinement chain")," used to obtain the summarized video and sample questions"),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"vector embedding chain")," that uses the LLM to generate text embeddings and store them in Redis")),(0,a.kt)("p",null,"The LLM ",(0,a.kt)("inlineCode",{parentName:"p"},"summary prompt")," is split into two parts. This is done to allow analyzing videos where the transcript length is larger than the LLM's accepted context."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/templates/video.ts"',title:'"services/video-search/src/api/templates/video.ts"'},"import { PromptTemplate } from 'langchain/prompts';\n\nconst summaryTemplate = `\nYou are an expert in summarizing YouTube videos.\nYour goal is to create a summary of a video.\nBelow you find the transcript of a video:\n--------\n{text}\n--------\n\nThe transcript of the video will also be used as the basis for a question and answer bot.\nProvide some examples questions and answers that could be asked about the video. Make these questions very specific.\n\nTotal output will be a summary of the video and a list of example questions the user could ask of the video.\n\nSUMMARY AND QUESTIONS:\n`;\n\nexport const SUMMARY_PROMPT = PromptTemplate.fromTemplate(summaryTemplate);\n\nconst summaryRefineTemplate = `\nYou are an expert in summarizing YouTube videos.\nYour goal is to create a summary of a video.\nWe have provided an existing summary up to a certain point: {existing_answer}\n\nBelow you find the transcript of a video:\n--------\n{text}\n--------\n\nGiven the new context, refine the summary and example questions.\nThe transcript of the video will also be used as the basis for a question and answer bot.\nProvide some examples questions and answers that could be asked about the video. Make\nthese questions very specific.\nIf the context isn't useful, return the original summary and questions.\nTotal output will be a summary of the video and a list of example questions the user could ask of the video.\n\nSUMMARY AND QUESTIONS:\n`;\n\nexport const SUMMARY_REFINE_PROMPT = PromptTemplate.fromTemplate(\n  summaryRefineTemplate,\n);\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"summary prompts")," are used to create a ",(0,a.kt)("inlineCode",{parentName:"p"},"refinement chain")," with LangChain. LangChain will automatically handle splitting the video transcript document(s) and calling the LLM accordingly."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/prompt.ts" {1-5,30-35}',title:'"services/video-search/src/api/prompt.ts"',"{1-5,30-35}":!0},"const videoSummarizeChain = loadSummarizationChain(llm, {\n  type: 'refine',\n  questionPrompt: SUMMARY_PROMPT,\n  refinePrompt: SUMMARY_REFINE_PROMPT,\n});\n\nconst summaryCache = cacheAside(`${prefix}-${config.redis.SUMMARY_PREFIX}`);\n\nasync function summarizeVideos(videos: VideoDocument[]) {\n  const summarizedDocs: VideoDocument[] = [];\n\n  for (const video of videos) {\n    log.debug(`Summarizing ${video.metadata.link}`, {\n      ...video.metadata,\n      location: `${prefix}.summarize.docs`,\n    });\n    const existingSummary = await summaryCache.get(video.metadata.id);\n\n    if (typeof existingSummary === 'string') {\n      summarizedDocs.push(\n        new Document({\n          metadata: video.metadata,\n          pageContent: existingSummary,\n        }),\n      );\n\n      continue;\n    }\n\n    const splitter = new TokenTextSplitter({\n      chunkSize: 10000,\n      chunkOverlap: 250,\n    });\n    const docsSummary = await splitter.splitDocuments([video]);\n    const summary = await videoSummarizeChain.run(docsSummary);\n\n    log.debug(`Summarized ${video.metadata.link}:\\n ${summary}`, {\n      summary,\n      location: `${prefix}.summarize.docs`,\n    });\n    await summaryCache.set(video.metadata.id, summary);\n\n    summarizedDocs.push(\n      new Document({\n        metadata: video.metadata,\n        pageContent: summary,\n      }),\n    );\n  }\n\n  return summarizedDocs;\n}\n")),(0,a.kt)("p",null,"Notice the ",(0,a.kt)("inlineCode",{parentName:"p"},"summaryCache")," is used to first ask Redis if the video has already been summarized. If it has, it will return the summary and skip the LLM. This is a great example of how Redis can be used to cache data and avoid unnecessary API calls. Below is an example video summary with questions."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="https://www.youtube.com/watch?v=LaiQFZ5bXaM"',title:'"https://www.youtube.com/watch?v'},"Summary:\nThe video provides a walkthrough of building a real-time stock tracking application\nusing Redis Stack, demonstrating its capability to handle multiple data models and\nact as a message broker in a single integrated database. The application maintains\na watch list of stock symbols, along with real-time trading information and a chart\nupdated with live data from the Alpaca API. The presenter uses Redis Stack features\nsuch as sets, JSON documents, time series, Pub/Sub, and Top-K filter to store and\nmanage different types of data. An architecture diagram is provided, explaining the\ninterconnection between the front end, API service, and streaming service within\nthe application. Code snippets highlight key aspects of the API and streaming\nservice written in Python, highlighting the use of Redis Bloom, Redis JSON, Redis\nTime Series, and Redis Search for managing data. The video concludes with a\ndemonstration of how data structures are visualized and managed in RedisInsight,\nemphasizing how Redis Stack can simplify the building of a complex real-time\napplication by replacing multiple traditional technologies with one solution.\n\nExample Questions and Answers:\n\nQ1: What is Redis Stack and what role does it play in the application?\nA1: Redis Stack is an extension to Redis that adds additional modules, turning it\ninto a multi-model database. In the application, it is used for storing various\ntypes of data and managing real-time communication between microservices.\n\nQ2: How is the stock watch list stored and managed within the application?\nA2: The watch list is stored as a Redis set which helps automatically prevent\nduplicate stock symbols. In addition, further information about each stock is\nstored in JSON documents within Redis Stack.\n\nQ3: What type of data does the application store using time series capabilities of\nRedis Stack?\nA3: The application uses time series data to store and retrieve the price movements\nof the stocks, making it easy to query over a date range and to visualize chart\ndata with time on the x-axis.\n\nQ4: Can you explain the use of the Top-K filter in the application?\nA4: The Top-K filter is a feature of Redis Bloom that is used to maintain a\nleaderboard of the most frequently traded stocks on the watch list, updating every\nminute with the number of trades that happen.\n\nQ5: What methods are used to update the front end with real-time information in\nthe application?\nA5: WebSockets are used to receive real-time updates for trending stocks, trades,\nand stock bars from the API service, which, in turn, receives the information from\nRedis Pub/Sub messages generated by the streaming service.\n\nQ6: How does the application sync the watch list with the streaming service?\nA6: The application listens to the watch list key space in Redis for updates. When\na stock is added or removed from the watch list on the front end, the API\ncommunicates this to Redis, and the streaming service then subscribes or\nunsubscribes to updates from the Alpaca API for that stock.\n\nQ7: What frontend technologies are mentioned for building the UI of the application?\nA7: The UI service for the front end is built using Tailwind CSS, Chart.js, and\nNext.js, which is a typical tech stack for creating a modern web application.\n\nQ8: How does Redis Insight help in managing the application data?\nA8: Redis Insight provides a visual interface to see and manage the data structures\nused in Redis Stack, including JSON documents, sets, and time series data related\nto the stock information in the application.\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"vector embedding chain")," is used to generate vector embeddings for the video summaries. This is done by asking the LLM to generate text embeddings for the summary. The ",(0,a.kt)("inlineCode",{parentName:"p"},"vector embedding chain")," is defined as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/store.ts"',title:'"services/video-search/src/api/store.ts"'},"const vectorStore = new RedisVectorStore(embeddings, {\n  redisClient: client,\n  indexName: `${prefix}-${config.redis.VIDEO_INDEX_NAME}`,\n  keyPrefix: `${prefix}-${config.redis.VIDEO_PREFIX}`,\n  indexOptions: {\n    ALGORITHM: VectorAlgorithms.HNSW,\n    DISTANCE_METRIC: 'IP',\n  },\n});\n")),(0,a.kt)("p",null,"The vector store uses the ",(0,a.kt)("inlineCode",{parentName:"p"},"RedisVectorStore")," class from LangChain. This class is a wrapper around Redis that allows you to store and search vector embeddings. We are using the ",(0,a.kt)("inlineCode",{parentName:"p"},"HNSW")," algorithm and the ",(0,a.kt)("inlineCode",{parentName:"p"},"IP")," distance metric. For more information on the supported algorithms and distance metrics, see the ",(0,a.kt)("a",{parentName:"p",href:"https://redis.io/docs/interact/search-and-query/advanced-concepts/vectors/"},"Redis vector store documentation"),". We pass the ",(0,a.kt)("inlineCode",{parentName:"p"},"embeddings")," object to the ",(0,a.kt)("inlineCode",{parentName:"p"},"RedisVectorStore")," constructor. This object is defined as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/llms/google.ts"',title:'"services/video-search/src/api/llms/google.ts"'},"new GoogleGenerativeAIEmbeddings({\n  apiKey: config.google.API_KEY,\n  modelName: modelName ?? config.google.EMBEDDING_MODEL,\n  taskType: TaskType.SEMANTIC_SIMILARITY,\n});\n")),(0,a.kt)("p",null,"Or for OpenAI:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/llms/openai.ts"',title:'"services/video-search/src/api/llms/openai.ts"'},"new OpenAIEmbeddings({\n  openAIApiKey: config.openai.API_KEY,\n  modelName: modelName ?? config.openai.EMBEDDING_MODEL,\n  configuration: {\n    organization: config.openai.ORGANIZATION,\n  },\n});\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"embeddings")," object is used to generate vector embeddings for the video summaries. These embeddings are then stored in Redis using the ",(0,a.kt)("inlineCode",{parentName:"p"},"vectorStore"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/store.ts" {28}',title:'"services/video-search/src/api/store.ts"',"{28}":!0},"async function storeVideoVectors(documents: VideoDocument[]) {\n  log.debug('Storing documents...', {\n    location: `${prefix}.store.store`,\n  });\n  const newDocuments: VideoDocument[] = [];\n\n  await Promise.all(\n    documents.map(async (doc) => {\n      const exists = await client.sIsMember(\n        `${prefix}-${config.redis.VECTOR_SET}`,\n        doc.metadata.id,\n      );\n\n      if (!exists) {\n        newDocuments.push(doc);\n      }\n    }),\n  );\n\n  log.debug(`Found ${newDocuments.length} new documents`, {\n    location: `${prefix}.store.store`,\n  });\n\n  if (newDocuments.length === 0) {\n    return;\n  }\n\n  await vectorStore.addDocuments(newDocuments);\n\n  await Promise.all(\n    newDocuments.map(async (doc) => {\n      await client.sAdd(\n        `${prefix}-${config.redis.VECTOR_SET}`,\n        doc.metadata.id,\n      );\n    }),\n  );\n}\n")),(0,a.kt)("p",null,"Notice that we first check if we have already generated a vector using the Redis Set ",(0,a.kt)("inlineCode",{parentName:"p"},"VECTOR_SET"),". If we have, we skip the LLM and use the existing vector. This avoids unnecessary API calls and can speed things up."),(0,a.kt)("h3",{id:"redis-vector-search-funcationality-and-ai-integration-for-video-qa"},"Redis vector search funcationality and AI integration for video Q&A"),(0,a.kt)("p",null,"One of the key features of our application is the ability to search through video content using AI-generated queries. This section will cover how the backend handles search requests and interacts with the AI models."),(0,a.kt)("h4",{id:"converting-questions-into-vectors"},"Converting questions into vectors"),(0,a.kt)("p",null,"When a user submits a question through the frontend, the backend performs the following steps to obtain the answer to the question as well as supporting videos:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"An ",(0,a.kt)("inlineCode",{parentName:"li"},"answerVectorStore")," is used to check if a similar question has already been answered. If so, we can skip the LLM and return the answer. This is called ",(0,a.kt)("inlineCode",{parentName:"li"},"semantic vector caching"),".",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"This step is optional, the user can choose to generate a unique answer every time."))),(0,a.kt)("li",{parentName:"ol"},"Assuming we need to generate a unique answer, we generate a semantically similar question to the one being asked. This helps to find the most relevant videos."),(0,a.kt)("li",{parentName:"ol"},"We then use the ",(0,a.kt)("inlineCode",{parentName:"li"},"vectorStore")," to search for the most relevant videos based on the semantic question."),(0,a.kt)("li",{parentName:"ol"},"If we don't find any relevant videos, we search with the original question."),(0,a.kt)("li",{parentName:"ol"},"Once we find videos, we call the LLM to answer the question."),(0,a.kt)("li",{parentName:"ol"},"We cache the answer and videos in Redis by generating a vector embedding for the original question and storing it along with the answer and videos."),(0,a.kt)("li",{parentName:"ol"},"Finally, we return the answer and supporting videos to the user.")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"answerVectorStore")," looks nearly identical to the ",(0,a.kt)("inlineCode",{parentName:"p"},"vectorStore")," we defined earlier, but it uses a different ",(0,a.kt)("a",{parentName:"p",href:"https://redis.io/docs/interact/search-and-query/advanced-concepts/vectors/"},"algorithm and disance metric"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/store.ts" {6-7}',title:'"services/video-search/src/api/store.ts"',"{6-7}":!0},"const answerVectorStore = new RedisVectorStore(embeddings, {\n  redisClient: client,\n  indexName: `${prefix}-${config.redis.ANSWER_INDEX_NAME}`,\n  keyPrefix: `${prefix}-${config.redis.ANSWER_PREFIX}`,\n  indexOptions: {\n    ALGORITHM: VectorAlgorithms.FLAT,\n    DISTANCE_METRIC: 'L2',\n  },\n});\n")),(0,a.kt)("p",null,"The following code demonstrates how to use the ",(0,a.kt)("inlineCode",{parentName:"p"},"answerVectorStore")," to check if a similar question has already been answered."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/search.ts" {16-19}',title:'"services/video-search/src/api/search.ts"',"{16-19}":!0},"async function checkAnswerCache(question: string) {\n  const haveAnswers = await answerVectorStore.checkIndexExists();\n\n  if (!(haveAnswers && config.searches.answerCache)) {\n    return;\n  }\n\n  log.debug(`Searching for closest answer to question: ${question}`, {\n    location: `${prefix}.search.getAnswer`,\n    question,\n  });\n\n  /**\n   * Scores will be between 0 and 1, where 0 is most accurate and 1 is least accurate\n   */\n  let results = (await answerVectorStore.similaritySearchWithScore(\n    question,\n    config.searches.KNN,\n  )) as Array<[AnswerDocument, number]>;\n\n  if (Array.isArray(results) && results.length > 0) {\n    // Filter out results with too high similarity score\n    results = results.filter(\n      (result) => result[1] <= config.searches.maxSimilarityScore,\n    );\n\n    const inaccurateResults = results.filter(\n      (result) => result[1] > config.searches.maxSimilarityScore,\n    );\n\n    if (Array.isArray(inaccurateResults) && inaccurateResults.length > 0) {\n      log.debug(\n        `Rejected ${inaccurateResults.length} similar answers that have a score > ${config.searches.maxSimilarityScore}`,\n        {\n          location: `${prefix}.search.getAnswer`,\n          scores: inaccurateResults.map((result) => result[1]),\n        },\n      );\n    }\n  }\n\n  if (Array.isArray(results) && results.length > 0) {\n    log.debug(\n      `Accepted ${results.length} similar answers that have a score <= ${config.searches.maxSimilarityScore}`,\n      {\n        location: `${prefix}.search.getAnswer`,\n        scores: results.map((result) => result[1]),\n      },\n    );\n\n    return results.map((result) => {\n      return {\n        ...result[0].metadata,\n        question: result[0].pageContent,\n        isOriginal: false,\n      };\n    });\n  }\n}\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"similaritySearchWithScore")," will find similar questions to the one being asked. It ranks them from ",(0,a.kt)("inlineCode",{parentName:"p"},"0")," to ",(0,a.kt)("inlineCode",{parentName:"p"},"1"),", where ",(0,a.kt)("inlineCode",{parentName:"p"},"0"),' is most similar or "closest". We then filter out any results that are too similar, as defined by the ',(0,a.kt)("inlineCode",{parentName:"p"},"maxSimilarityScore")," environment variable. If we find any results, we return them to the user. Using a max score is crucial here, because we don't want to return inaccurate results."),(0,a.kt)("p",null,"If we don't find answers in the ",(0,a.kt)("inlineCode",{parentName:"p"},"semantic vector cache")," then we need to generate a unique answer. This is done by first generating a semantically similar question to the one being asked. This is done using the ",(0,a.kt)("inlineCode",{parentName:"p"},"QUESTION_PROMPT")," defined below:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/templates/questions.ts"',title:'"services/video-search/src/api/templates/questions.ts"'},"import { PromptTemplate } from 'langchain/prompts';\n\nconst questionTemplate = `\nYou are an expert in summarizing questions.\nYour goal is to reduce a question down to its simplest form while still retaining the semantic meaning.\nBelow you find the question:\n--------\n{question}\n--------\n\nTotal output will be a semantically similar question that will be used to search an existing dataset.\n\nSEMANTIC QUESTION:\n`;\n\nexport const QUESTION_PROMPT = PromptTemplate.fromTemplate(questionTemplate);\n")),(0,a.kt)("p",null,"Using this prompt, we generate the ",(0,a.kt)("inlineCode",{parentName:"p"},"semantic question")," and use it to search for videos. We may also need to search using the original ",(0,a.kt)("inlineCode",{parentName:"p"},"question")," if we don't find any videos with the ",(0,a.kt)("inlineCode",{parentName:"p"},"semantic question"),". This is done using the ",(0,a.kt)("inlineCode",{parentName:"p"},"ORIGINAL_QUESTION_PROMPT")," defined below:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/search.ts" {12-14,33,38,48,55,58,61-67}',title:'"services/video-search/src/api/search.ts"',"{12-14,33,38,48,55,58,61-67}":!0},"async function getVideos(question: string) {\n  log.debug(\n    `Performing similarity search for videos that answer: ${question}`,\n    {\n      question,\n      location: `${prefix}.search.search`,\n    },\n  );\n\n  const KNN = config.searches.KNN;\n  /* Simple standalone search in the vector DB */\n  return await (vectorStore.similaritySearch(question, KNN) as Promise<\n    VideoDocument[]\n  >);\n}\n\nasync function searchVideos(\n  question: string,\n  { useCache = config.searches.answerCache }: VideoSearchOptions = {},\n) {\n  log.debug(`Original question: ${question}`, {\n    location: `${prefix}.search.search`,\n  });\n\n  if (useCache) {\n    const existingAnswer = await checkAnswerCache(question);\n\n    if (typeof existingAnswer !== 'undefined') {\n      return existingAnswer;\n    }\n  }\n\n  const semanticQuestion = await prompt.getSemanticQuestion(question);\n\n  log.debug(`Semantic question: ${semanticQuestion}`, {\n    location: `${prefix}.search.search`,\n  });\n  let videos = await getVideos(semanticQuestion);\n\n  if (videos.length === 0) {\n    log.debug(\n      'No videos found for semantic question, trying with original question',\n      {\n        location: `${prefix}.search.search`,\n      },\n    );\n\n    videos = await getVideos(question);\n  }\n\n  log.debug(`Found ${videos.length} videos`, {\n    location: `${prefix}.search.search`,\n  });\n\n  const answerDocument = await prompt.answerQuestion(question, videos);\n\n  if (config.searches.answerCache) {\n    await answerVectorStore.addDocuments([answerDocument]);\n  }\n\n  return [\n    {\n      ...answerDocument.metadata,\n      question: answerDocument.pageContent,\n      isOriginal: true,\n    },\n  ];\n}\n")),(0,a.kt)("p",null,"The code above shows the whole process, from checking the ",(0,a.kt)("inlineCode",{parentName:"p"},"semantic vector cache")," for existing answers, all the way to getting answers from the LLM and caching them in Redis for future potential questions. Once relevant videos are identified, the backend uses either Google Gemini or OpenAI's ChatGPT to generate answers. These answers are formulated based on the video transcripts stored in Redis, ensuring they are contextually relevant to the user's query. The ",(0,a.kt)("inlineCode",{parentName:"p"},"ANSWER_PROMPT")," used to ask the LLM for answers is as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-typescript",metastring:'title="services/video-search/src/api/templates/answers.ts"',title:'"services/video-search/src/api/templates/answers.ts"'},"import { PromptTemplate } from 'langchain/prompts';\n\nconst answerTemplate = `\nYou are an expert in answering questions about Redis and Redis Stack.\nYour goal is to take a question and some relevant information extracted from videos and return the answer to the question.\n\n- Try to mostly use the provided video info, but if you can't find the answer there you can use other resources.\n- Make sure your answer is related to Redis. All questions are about Redis. For example, if a question is asking about strings, it is asking about Redis strings.\n- The answer should be formatted as a reference document using markdown. Make all headings and links bold, and add new paragraphs around any code blocks.\n- Your answer should include as much detail as possible and be no shorter than 500 words.\n\nHere is some extracted video information relevant to the question: {data}\n\nBelow you find the question:\n--------\n{question}\n--------\n\nTotal output will be the answer to the question.\n\nANSWER:\n`;\n\nexport const ANSWER_PROMPT = PromptTemplate.fromTemplate(answerTemplate);\n")),(0,a.kt)("p",null,"That's it! The backend will now return the answer and supporting videos to the user. Not included in this tutorial is an overview of the frontend ",(0,a.kt)("inlineCode",{parentName:"p"},"Next.js")," app. However, you can find the code in the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/wjohnsto/genai-qa-videos"},"GitHub repository")," in the ",(0,a.kt)("inlineCode",{parentName:"p"},"app")," directory."),(0,a.kt)("p",null,"Below are a couple screenshots from the application to see what it looks like when you find an existing answer to a question:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Existing video answer",src:n(91050).Z,width:"3060",height:"1308"})),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Supporting videos",src:n(2938).Z,width:"2950",height:"1226"})),(0,a.kt)("h2",{id:"conclusion"},"Conclusion"),(0,a.kt)("p",null,"In this tutorial, we've explored how to build an AI-powered video Q&A application using Redis, LangChain, and various other technologies. We've covered setting up the environment, processing video uploads, and implementing search functionality. You also saw how to use Redis as a ",(0,a.kt)("inlineCode",{parentName:"p"},"vector store")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"semantic vector cache"),"."),(0,a.kt)("h3",{id:"key-takeaways"},"Key takeaways"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Generative AI can be leveraged to create powerful applications without writing a ton of code."),(0,a.kt)("li",{parentName:"ul"},"Redis is highly versatile and efficient in handling AI-generated data and vectors."),(0,a.kt)("li",{parentName:"ul"},"LangChain makes it easy to integrate AI models with vector stores.")),(0,a.kt)("p",null,"Remember, Redis offers an easy start with cloud-hosted instances, which you can sign up for at ",(0,a.kt)("a",{parentName:"p",href:"https://redis.com/try-free"},"redis.com/try-free"),". This makes experimenting with AI and Redis more accessible than ever."),(0,a.kt)("p",null,"We hope this tutorial inspires you to explore the exciting possibilities of combining AI with powerful databases like Redis to create innovative applications."),(0,a.kt)("h2",{id:"further-reading"},"Further reading"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/howtos/solutions/vector/getting-started-vector"},"Perform vector similarity search using Redis")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/howtos/solutions/vector/gen-ai-chatbot"},"Building a generative AI chatbot using Redis")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://js.langchain.com/docs/get_started/quickstart"},"LangChain JS"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://scrimba.com/learn/langchain"},"Learn LangChain")))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://js.langchain.com/docs/integrations/vectorstores/redis"},"LangChain Redis integration"))))}h.isMDXComponent=!0},21540:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/ask-question-1b9dbaafd33a3a4a0476a0705aed5e78.png"},37800:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/redisinsight-keys-e107b0b36375c633d4774719128082e3.png"},77723:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/upload-videos-2886a310e21fe43fea0f8471bb6e7863.png"},91050:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/video-qa-existing-answer-4da04f2c841af515ca5857b7a6dc8965.png"},2938:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/video-qa-supporting-videos-c7baf9dd3a17841ebde53b1b9dc159a6.png"}}]);